{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# HabitLedger Demo â€“ Behavioural Money Coach\n",
    "\n",
    "Welcome to the **HabitLedger** interactive demonstration notebook!\n",
    "\n",
    "## What is HabitLedger?\n",
    "\n",
    "HabitLedger is an AI-powered behavioural money coach designed to help users build healthier financial habits through:\n",
    "\n",
    "- **Behavioural Science Principles**: Uses proven concepts like loss aversion, habit loops, friction reduction, and commitment devices\n",
    "- **Personalized Interventions**: Suggests specific, actionable strategies tailored to your situation\n",
    "- **Memory & Tracking**: Remembers your goals, streaks, and struggles across interactions\n",
    "- **Adaptive Coaching**: Detects patterns and adjusts recommendations over time\n",
    "\n",
    "## What This Notebook Demonstrates\n",
    "\n",
    "This notebook showcases:\n",
    "\n",
    "1. How the agent analyzes user input using keyword-based principle detection\n",
    "2. How it generates personalized coaching responses with interventions\n",
    "3. How memory tracks progress across multiple interactions\n",
    "4. Sample scenarios covering common financial habit challenges\n",
    "\n",
    "**Note**: This demo uses deterministic keyword matching (no LLM calls yet) to demonstrate the core agent architecture and behaviour analysis logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## How This Notebook Is Organized\n",
    "\n",
    "1. **Setup** â€” Import modules, load behaviour database, initialize user memory\n",
    "2. **Single Interaction Demo** â€” See how the agent responds to one user input\n",
    "3. **Multiple Interaction Scenarios** â€” Run through 3-5 predefined scenarios covering different behavioural principles\n",
    "4. **Visuals & Structured Summaries** â€” Placeholder for future visualizations (e.g., streak charts, principle frequency)\n",
    "5. **Evaluation Notes** â€” How to assess the agent's performance and relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import modules, load the behaviour database, and initialize user memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.memory import UserMemory\n",
    "from src.behaviour_engine import analyse_behaviour, load_behaviour_db\n",
    "from src.coach import run_once\n",
    "\n",
    "# Load behaviour principles database\n",
    "behaviour_db = load_behaviour_db(\"../data/behaviour_principles.json\")\n",
    "print(f\"âœ… Loaded {len(behaviour_db)} behavioural principles\")\n",
    "\n",
    "# Initialize user memory\n",
    "user_memory = UserMemory(user_id=\"demo_user\")\n",
    "user_memory.goals = [\"Save $500/month\", \"Stop impulse buying\"]\n",
    "print(f\"âœ… Initialized memory for user: {user_memory.user_id}\")\n",
    "print(f\"   Goals: {user_memory.goals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Single Interaction Demo\n",
    "\n",
    "Let's see how the agent responds to a single user input. The agent will:\n",
    "1. Analyze the input for behavioural patterns\n",
    "2. Detect the most relevant principle\n",
    "3. Suggest personalized interventions\n",
    "4. Update the user's memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user input\n",
    "user_input = \"I keep buying coffee every morning even though I want to save money. It's become such a routine.\"\n",
    "\n",
    "# Run the agent\n",
    "response = run_once(\n",
    "    user_input=user_input, memory=user_memory, behaviour_db=behaviour_db\n",
    ")\n",
    "\n",
    "print(\"ðŸ¤– Agent Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Multiple Interaction Scenarios\n",
    "\n",
    "Now let's test the agent with several predefined scenarios that trigger different behavioural principles. This demonstrates how the agent adapts to different situations and tracks progress over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple test scenarios\n",
    "scenarios = [\n",
    "    {\n",
    "        \"name\": \"Loss Aversion\",\n",
    "        \"input\": \"I'm afraid to check my bank account because I might see I've overspent again. It makes me anxious.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Temptation Bundling\",\n",
    "        \"input\": \"I hate budgeting, it feels like such a chore. I keep putting it off.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Friction Reduction (Negative)\",\n",
    "        \"input\": \"Online shopping is too easy. I just click and buy without thinking. No friction at all.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Micro Habits\",\n",
    "        \"input\": \"Saving $500 a month feels overwhelming. I don't know where to start.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Default Effect\",\n",
    "        \"input\": \"I'm still subscribed to services I never use. I just haven't bothered to cancel them.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run through each scenario\n",
    "print(\"=\" * 60)\n",
    "for i, scenario in enumerate(scenarios, 1):\n",
    "    print(f\"\\nðŸ“Œ SCENARIO {i}: {scenario['name']}\")\n",
    "    print(f\"User: {scenario['input']}\")\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "    response = run_once(\n",
    "        user_input=scenario[\"input\"], memory=user_memory, behaviour_db=behaviour_db\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Visuals and Structured Summaries (Placeholder)\n",
    "\n",
    "This section is a placeholder for future visualizations and analytics:\n",
    "\n",
    "- **Streak Charts**: Visualize consistency over time\n",
    "- **Principle Frequency**: Show which behavioural patterns are most common\n",
    "- **Progress Dashboard**: Track goal completion and intervention effectiveness\n",
    "- **Behaviour Heatmaps**: Identify patterns by day/time\n",
    "\n",
    "For now, we can inspect the memory state directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect current memory state\n",
    "import json\n",
    "\n",
    "memory_state = user_memory.to_dict()\n",
    "print(\"ðŸ“Š Current User Memory State:\")\n",
    "print(json.dumps(memory_state, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5. Evaluation Notes\n",
    "\n",
    "### How to Evaluate HabitLedger\n",
    "\n",
    "When assessing the agent's performance, consider:\n",
    "\n",
    "#### 1. **Principle Detection Accuracy**\n",
    "- Does the agent correctly identify the underlying behavioural principle?\n",
    "- Are the detected triggers relevant to the user's input?\n",
    "\n",
    "#### 2. **Intervention Relevance**\n",
    "- Are the suggested interventions actionable and specific?\n",
    "- Do they align with the detected principle and user's goals?\n",
    "\n",
    "#### 3. **Memory & Consistency**\n",
    "- Does the agent remember goals, streaks, and struggles across interactions?\n",
    "- Does it reference past context appropriately?\n",
    "\n",
    "#### 4. **Response Quality**\n",
    "- Is the tone supportive and non-judgmental?\n",
    "- Are explanations clear and grounded in behavioural science?\n",
    "\n",
    "#### 5. **Multi-Step Behavior**\n",
    "- Does the agent show adaptive behavior over multiple interactions?\n",
    "- Does it recognize patterns and adjust recommendations?\n",
    "\n",
    "### Current Limitations\n",
    "\n",
    "- **Keyword-based detection**: Not yet using LLM for nuanced pattern recognition\n",
    "- **No real-time data**: Memory persists within session but doesn't integrate with actual financial data\n",
    "- **Limited context**: Doesn't yet maintain conversation history beyond memory fields\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "- Integrate LLM-based principle detection for better accuracy\n",
    "- Add visualizations for streak tracking and progress monitoring\n",
    "- Implement session summaries and weekly reviews\n",
    "- Connect to financial APIs for real-time habit tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. Evaluation: Run Scenario Set\n",
    "\n",
    "This cell runs a comprehensive set of test scenarios and produces a structured summary table showing:\n",
    "- Detected principle for each prompt\n",
    "- Source of detection (ADK or keyword)\n",
    "- Intervention provided\n",
    "- Response length\n",
    "\n",
    "This helps evaluate:\n",
    "- **Accuracy**: Are the right principles detected?\n",
    "- **Coverage**: Does the agent handle diverse scenarios?\n",
    "- **Consistency**: Are responses appropriately detailed?\n",
    "- **Performance**: Which detection method is used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Fresh memory for evaluation\n",
    "eval_memory = UserMemory(user_id=\"eval_user\")\n",
    "eval_memory.goals = [{\"description\": \"Build better financial habits\"}]\n",
    "\n",
    "# Comprehensive test scenarios covering all principles\n",
    "test_scenarios = [\n",
    "    # Loss Aversion\n",
    "    \"I'm afraid to check my savings account because I might see I've spent too much\",\n",
    "    \"I broke my 30-day no-spending streak yesterday and feel terrible about it\",\n",
    "    \"I regret buying that expensive gadget last week\",\n",
    "    # Habit Loops\n",
    "    \"Every evening after work I automatically order food delivery when I'm stressed\",\n",
    "    \"Whenever I feel bored, I start browsing shopping apps\",\n",
    "    \"I always grab coffee at the same cafe during my lunch break\",\n",
    "    # Commitment Devices\n",
    "    \"I need help sticking to my budget, my willpower isn't enough\",\n",
    "    \"I keep forgetting to transfer money to my savings account\",\n",
    "    \"It's hard to resist temptation when sales pop up\",\n",
    "    # Temptation Bundling\n",
    "    \"Reviewing my expenses is so boring and tedious\",\n",
    "    \"I dread looking at my budget spreadsheet\",\n",
    "    # Friction Reduction (for good habits)\n",
    "    \"Tracking my expenses takes too many steps and is confusing\",\n",
    "    \"Setting up automatic savings is too complicated\",\n",
    "    # Friction Increase (for bad habits)\n",
    "    \"One-click ordering makes it too easy to impulse buy\",\n",
    "    \"I keep ordering food delivery because the app is right on my phone\",\n",
    "    \"Online shopping is instant, I don't even think about it\",\n",
    "    # Default Effect\n",
    "    \"I forget to save money each month, I need to automate it\",\n",
    "    \"I'm still paying for subscriptions I never use\",\n",
    "    # Micro Habits\n",
    "    \"Saving $1000 a month feels overwhelming and impossible\",\n",
    "    \"My financial goals are too big, I don't know where to start\",\n",
    "]\n",
    "\n",
    "print(f\"ðŸ§ª Running {len(test_scenarios)} evaluation scenarios...\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "results = []\n",
    "start_eval_time = time.time()\n",
    "\n",
    "for idx, prompt in enumerate(test_scenarios, 1):\n",
    "    # Analyze the prompt\n",
    "    analysis = analyse_behaviour(prompt, eval_memory, behaviour_db)\n",
    "\n",
    "    # Extract data\n",
    "    principle_id = analysis.get(\"detected_principle_id\", \"None\")\n",
    "    source = analysis.get(\"source\", \"unknown\")\n",
    "    interventions = analysis.get(\"intervention_suggestions\", [])\n",
    "    intervention_text = interventions[0] if interventions else \"No intervention\"\n",
    "\n",
    "    # Get full response\n",
    "    response = run_once(prompt, eval_memory, behaviour_db)\n",
    "    response_length = len(response)\n",
    "\n",
    "    # Truncate for display\n",
    "    prompt_short = prompt[:60] + \"...\" if len(prompt) > 60 else prompt\n",
    "    intervention_short = (\n",
    "        intervention_text[:50] + \"...\"\n",
    "        if len(intervention_text) > 50\n",
    "        else intervention_text\n",
    "    )\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"ID\": idx,\n",
    "            \"Prompt\": prompt_short,\n",
    "            \"Principle\": principle_id if principle_id else \"None\",\n",
    "            \"Source\": source,\n",
    "            \"Intervention\": intervention_short,\n",
    "            \"Response Len\": response_length,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Show progress\n",
    "    if idx % 5 == 0:\n",
    "        print(f\"  âœ“ Processed {idx}/{len(test_scenarios)} scenarios\")\n",
    "\n",
    "eval_duration = time.time() - start_eval_time\n",
    "\n",
    "print(f\"\\nâœ… Evaluation complete in {eval_duration:.2f}s\")\n",
    "print(f\"ðŸ“Š Summary Table:\\n\")\n",
    "\n",
    "# Create DataFrame and display\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nðŸ“ˆ Evaluation Metrics:\")\n",
    "print(f\"  â€¢ Total scenarios: {len(test_scenarios)}\")\n",
    "print(f\"  â€¢ Principles detected: {df_results['Principle'].notna().sum()}\")\n",
    "print(f\"  â€¢ ADK detections: {(df_results['Source'] == 'adk').sum()}\")\n",
    "print(f\"  â€¢ Keyword detections: {(df_results['Source'] == 'keyword').sum()}\")\n",
    "print(f\"  â€¢ Avg response length: {df_results['Response Len'].mean():.0f} chars\")\n",
    "print(f\"  â€¢ Total evaluation time: {eval_duration:.2f}s\")\n",
    "print(f\"  â€¢ Avg time per scenario: {eval_duration/len(test_scenarios):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Evaluation Metrics Explained\n",
    "\n",
    "The table above shows results from running the agent on diverse test scenarios. Here's how to interpret the metrics:\n",
    "\n",
    "#### Qualitative Metrics (Manual Assessment)\n",
    "\n",
    "1. **Clarity**: Are the agent's responses clear and easy to understand?\n",
    "   - Check if principle names are explained\n",
    "   - Verify interventions are actionable and specific\n",
    "\n",
    "2. **Relevance**: Do detected principles match the user's actual issue?\n",
    "   - Compare the prompt to the detected principle\n",
    "   - Assess if interventions address the root cause\n",
    "\n",
    "3. **Tone**: Is the agent supportive and non-judgmental?\n",
    "   - Look for encouraging language\n",
    "   - Check that responses avoid blame or criticism\n",
    "\n",
    "#### Automated Metrics\n",
    "\n",
    "1. **Source Distribution**: Shows whether ADK (LLM) or keyword matching was used\n",
    "   - Higher ADK usage suggests more nuanced detection\n",
    "   - Keyword fallback ensures robustness\n",
    "\n",
    "2. **Response Length**: Indicates level of detail in responses\n",
    "   - Typical range: 300-800 characters\n",
    "   - Too short may lack detail; too long may be overwhelming\n",
    "\n",
    "3. **Processing Time**: Measures performance\n",
    "   - ADK calls are slower (~1-3s) but more accurate\n",
    "   - Keyword matching is faster (~0.1s) but less nuanced\n",
    "\n",
    "4. **Coverage**: Percentage of scenarios where a principle was detected\n",
    "   - Target: >90% principle detection rate\n",
    "   - \"None\" detections indicate unclear input or gaps in coverage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
